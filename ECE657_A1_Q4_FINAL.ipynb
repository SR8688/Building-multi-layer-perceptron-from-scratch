{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "BNRmqPbhIROK",
    "outputId": "e87bec87-4261-43e2-82c4-375008af12f2"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6skRZ1UIaze"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nk6H0UptIgwM"
   },
   "outputs": [],
   "source": [
    "# #Loading Data set\n",
    "# data = genfromtxt(\"/content/drive/My Drive/ECE657_assign1/train_data.csv\", delimiter=',')\n",
    "# label=genfromtxt(\"/content/drive/My Drive/ECE657_assign1/train_labels.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data set\n",
    "data = genfromtxt(\"Data/train_data.csv\", delimiter=',')\n",
    "label=genfromtxt(\"Data/train_labels.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7d776f99a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[1500, :].reshape((28, 28)),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "j_tTA1AbVwhr",
    "outputId": "387e6a43-aad7-413a-908a-3c9504939a4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in each class [5923. 6742. 5958. 6131.]\n"
     ]
    }
   ],
   "source": [
    "#checking the balance the dataset\n",
    "print('Number of samples in each class', label.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x5LuxlOcWXfo",
    "outputId": "0c20d098-7b5f-4f1e-ac2c-33b4467f0c6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSo the class0 has 5923 samples\\n   the class1 has 6742 samples\\n   the class2 has 5958 samples\\n   the class3 has 6131 samples\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So the class0 has 5923 samples\n",
    "   the class1 has 6742 samples\n",
    "   the class2 has 5958 samples\n",
    "   the class3 has 6131 samples\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tb3LyP4gI-vD"
   },
   "outputs": [],
   "source": [
    "#Splitting the data into training and validation set in a ratio of 8:2\n",
    "#Created validation set to test the model on unknown data\n",
    "train_data=data[:19800,:]\n",
    "train_label=label[:19800,:]\n",
    "validation_data=data[19800:,:]\n",
    "validation_label=label[19800:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "zcgzT6e8YGhw",
    "outputId": "1efba4c9-13a5-4569-d241-c7a2cad41110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in each class in train label [4721. 5453. 4748. 4878.]\n",
      "Number of samples in each class in validation label [1202. 1289. 1210. 1253.]\n"
     ]
    }
   ],
   "source": [
    "#checking the balance the dataset\n",
    "print('Number of samples in each class in train label', train_label.sum(axis=0))\n",
    "print('Number of samples in each class in validation label', validation_label.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c1eGNuGSZljj",
    "outputId": "d3dd4bbe-c7ac-430d-d767-af29fe671049"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSo number of samples for each class for train label and validation label is almost balanced.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So number of samples for each class for train label and validation label is almost balanced.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "guW7rGS_JCDJ"
   },
   "outputs": [],
   "source": [
    "# finding out the number of samples and features in train dataset\n",
    "(samples,features) = train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1FWytYv9JDcU"
   },
   "outputs": [],
   "source": [
    "#declaring the number of hidden layer neurons, classes, learning rate and number of epoch for the training\n",
    "'''\n",
    "using 25 hidden layer neurons I was getting 97% accuracy on validation set.\n",
    "I increased the hidden layer to 125 to test whether I was getting \n",
    "good accuracy or not. It increased the accuracy a bit but was taking\n",
    "more computation time. So I took 25 hidden layer neurons.\n",
    "'''\n",
    "hidden_lyr_nodes = 25 \n",
    "num_classes = 4\n",
    "learning_rate = 10e-5\n",
    "num_epoch=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z99nnvVsIoIg"
   },
   "outputs": [],
   "source": [
    "# defined sigmoid function\n",
    "def sigmoid_func(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "# definded the derivative of sigmoid function\n",
    "def sigmoid_drv(x):\n",
    "    return sigmoid_func(x) * (1 - sigmoid_func(x))\n",
    "# defined softmax fucntion\n",
    "def softmax_func(x):\n",
    "    val = np.exp(x) / np.exp(x).sum(axis=1, keepdims=True)\n",
    "    return val\n",
    "# Turned predicted data into one hot encoding\n",
    "def one_hot_enc(x):\n",
    "  for i in range(0,len(x)):\n",
    "    x[i,x[i,:].argmax()]=1\n",
    "  out = (x == 1).astype(float)\n",
    "  return out\n",
    "# defined a fucntion for predicting the accuracy of the model\n",
    "def prediction(x,y):\n",
    "  corr=0\n",
    "  incorr=0\n",
    "  for i in range(0,len(x)):\n",
    "    if np.array_equiv(x[i,:] , y[i,:]):\n",
    "      corr+=1\n",
    "    else:\n",
    "      incorr+=1\n",
    "  accuracy=(corr/(corr+incorr))\n",
    "  return accuracy\n",
    "\n",
    "# defined a feed forward function\n",
    "def fwd(inp_data, wt_hid_lyr, bias_hid_lyr,wt_out_lyr,bias_out_lyr):\n",
    "    '''\n",
    "    calculating the hidden weighted input for each neuron in hidden layer \n",
    "    by multiplying input with hidden weights of that neuron\n",
    "    and adding the hidden bias values\n",
    "    '''\n",
    "    net_hidden = np.dot(inp_data, wt_hid_lyr) + bias_hid_lyr\n",
    "    '''\n",
    "    calculating the hidden activations using sigmoid funtion\n",
    "    '''\n",
    "    act_hidden = sigmoid_func(net_hidden)\n",
    "    '''\n",
    "    calculating the ooutput weighted input for each neuron in output layer \n",
    "    by multiplying hidden activations with output weights of that neuron\n",
    "    and adding the output bias values\n",
    "    '''\n",
    "    net_output = np.dot(act_hidden, wt_out_lyr) + bias_out_lyr\n",
    "    '''\n",
    "    calculating the output activations using softmax funtion\n",
    "    '''\n",
    "    act_output = softmax_func(net_output)\n",
    "    return act_output, act_hidden, net_hidden\n",
    "\n",
    "\n",
    "def bkd(train_data, train_label, net_hidden, act_hidden, weight_output, act_output):\n",
    "    '''\n",
    "    Calculating the output layer error.\n",
    "    Here, act_output is the predicted output.\n",
    "    train_label is the original label.\n",
    "    finding the derivative of the cost function with respect to \n",
    "    weights in the output layer\n",
    "    '''\n",
    "    cf_netHid = act_output - train_label \n",
    "    grad_bias_out = cf_netHid\n",
    "    grad_wt_out = np.dot(act_hidden.T, cf_netHid)\n",
    "    '''\n",
    "    Calculating the hidden layer error\n",
    "    finding the derivative of the cost function with respect to \n",
    "    weights in the hidden layer\n",
    "    '''\n",
    "    cf_actHid = np.dot(cf_netHid, weight_output.T)\n",
    "    grad_wt_hid = np.dot(train_data.T, sigmoid_drv(net_hidden) * cf_actHid)\n",
    "    grad_bias_hid = cf_actHid * sigmoid_drv(net_hidden)\n",
    "\n",
    "\n",
    "    return grad_wt_out, grad_bias_out, grad_wt_hid, grad_bias_hid\n",
    "\n",
    "#defined a function for weight updating\n",
    "def update_weigh(weight, cost):\n",
    "    if cost.shape == (features, hidden_lyr_nodes) or cost.shape == (hidden_lyr_nodes, num_classes):\n",
    "        weight -= learning_rate * cost\n",
    "    elif cost.shape == (samples, hidden_lyr_nodes) or cost.shape == (samples, num_classes):\n",
    "        weight -= learning_rate * cost.sum(axis=0)\n",
    "    return weight\n",
    "def cross_ent(original_label,predict_label):\n",
    "  mul=-original_label * np.log(predict_label)\n",
    "  val= np.sum(mul)\n",
    "  return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCYgaEr9ooG4"
   },
   "source": [
    "I used sigmoid function as activation function of the hidden layer neurons. For the output neurons, I used softmax function. The choice of activaiton function is a design choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNZj0sSkI27t"
   },
   "outputs": [],
   "source": [
    "# Weight initialization\n",
    "weight_hidden = np.random.randn(features, hidden_lyr_nodes)\n",
    "bias_hidden = np.random.randn(hidden_lyr_nodes)\n",
    "weight_output = np.random.randn(hidden_lyr_nodes, num_classes)\n",
    "bias_output = np.random.randn(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rF8sserVJTab",
    "outputId": "4c4d3f55-3af4-4ffb-8aa3-fedf90dc27f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  100     Loss function value:  3921.991105759267 accuracy =  0.9418651594670973\n",
      "epoch =  200     Loss function value:  2941.0681194113586 accuracy =  0.9527654420670165\n",
      "epoch =  300     Loss function value:  2477.9118608769063 accuracy =  0.959628582963262\n"
     ]
    }
   ],
   "source": [
    "error_per_epoch = list()\n",
    "epoch=0\n",
    "while epoch<num_epoch:\n",
    "  epoch+=1\n",
    "  #forward propagation\n",
    "  act_output, act_hidden, net_hidden = fwd(train_data, weight_hidden, bias_hidden, weight_output, bias_output)\n",
    "  #backward propagation\n",
    "  dcost_wo, dcost_bo, dcost_wh, dcost_bh = bkd(train_data, train_label, net_hidden, act_hidden, weight_output, act_output)\n",
    "  #weight updating\n",
    "  weight_hidden = update_weigh(weight_hidden, dcost_wh)\n",
    "  bias_hidden = update_weigh(bias_hidden, dcost_bh)\n",
    "  weight_output = update_weigh(weight_output, dcost_wo)\n",
    "  bias_output = update_weigh(bias_output, dcost_bo)\n",
    "  \n",
    "  cal_loss = cross_ent(train_label,act_output)\n",
    "  error_per_epoch.append(cal_loss)\n",
    "  #calculating the accuracy per epoch\n",
    "  y_pred, _, _ = fwd(validation_data, weight_hidden, bias_hidden, weight_output, bias_output)\n",
    "  y_pred_enc=one_hot_enc(y_pred)\n",
    "  acc=prediction(validation_label,y_pred_enc)\n",
    "  if epoch%100==0:\n",
    "    print('epoch = ',epoch,'   ','Loss function value: ', cal_loss,'accuracy = ',acc)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "40kbjduHJbr3",
    "outputId": "e118cf34-2112-4e13-9d63-a20e21fc1b46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.24001254e-04, 1.58635648e-04, 9.99510327e-01, 2.07035836e-04],\n",
       "       [8.48189222e-05, 6.82236175e-04, 1.12707698e-02, 9.87962175e-01],\n",
       "       [9.99624582e-01, 1.04484670e-06, 1.74432013e-04, 1.99941394e-04],\n",
       "       ...,\n",
       "       [1.95069251e-03, 1.07223415e-05, 9.97016605e-01, 1.02198014e-03],\n",
       "       [9.61540435e-07, 9.93754584e-01, 3.62989564e-03, 2.61455904e-03],\n",
       "       [8.94508734e-05, 8.63997483e-04, 5.28505794e-03, 9.93761494e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, _, _ = fwd(validation_data, weight_hidden, bias_hidden, weight_output, bias_output)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4Tsh8gRJr9a"
   },
   "outputs": [],
   "source": [
    "y_pred_enc=one_hot_enc(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "leI5FjqlJuGf",
    "outputId": "ab2ef635-80cc-47c1-d16b-776cf3d49101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on the validation set is  95.9628582963262\n"
     ]
    }
   ],
   "source": [
    "acc=prediction(validation_label,y_pred_enc)\n",
    "print('accuracy on the validation set is ', acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kl0VFyuEtxCJ"
   },
   "source": [
    "So the accuracy on the validation set is around 95.96%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Dbl5cLBRHhh"
   },
   "outputs": [],
   "source": [
    "np.save('Saved Weights/weight_hidden.npy', weight_hidden)\n",
    "np.save('Saved Weights/bias_hidden.npy', bias_hidden)\n",
    "np.save('Saved Weights/weight_output.npy', weight_output)\n",
    "np.save('Saved Weights/bias_output.npy', bias_output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ECE657_A1_Q4_FINAL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
